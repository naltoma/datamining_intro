{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 簡易版転移学習の例\n",
    "- 関連記事\n",
    "    - [テキストデータに対する特徴量設計3（転移学習）](./nlp3.md)\n",
    "- 本当にやりたいタスクは [The 20 newsgroups text dataset](https://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset) の分類タスク。\n",
    "- 簡易版転移学習アプローチの例\n",
    "    - pre-training stage\n",
    "        - [gensim(fastText)](https://radimrehurek.com/gensim/) で、[swwiki-latest-pages-articles.xml.bz2](https://dumps.wikimedia.org/swwiki/latest/swwiki-latest-pages-articles.xml.bz2) から言語モデルを学習。\n",
    "    - fine tuning stage\n",
    "        - 言語モデルを使ってニュース記事をベクトル化。このベクトルを使って分類学習する。\n",
    "- 比較対象\n",
    "    - 直接 BoW + TFIDF で分類学習する。他のコーパスを使わず、同じコーパス内での他タスクもしない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Use model file \"/Users/tnal/PycharmProjects/dm_nlp/transfer_learning/gensim/swwiki\"?\n",
      "[y/n] => y\n",
      "[-0.47669616 -0.30191633  0.36801535 -0.68636864  0.9953687 ]\n",
      "[-0.06122479 -0.10474044  0.15912144 -0.24299866 -0.24580786]\n"
     ]
    }
   ],
   "source": [
    "# pre-training stage.\n",
    "# building dataset from swwiki-latest-pages-articles.xml.bz2, and train a model with fastText\n",
    "import os, pickle, re, json\n",
    "import multiprocessing\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "\n",
    "# 事前学習用のコーパス、学習後のモデルを保存するファイル名の指定\n",
    "#wikipedia_data = '/Users/tnal/Downloads/data/wikipedia-en/swwiki-latest-pages-articles.xml.bz2'\n",
    "#path = \"/Users/tnal/PycharmProjects/dm_nlp/transfer_learning/gensim\"\n",
    "#model_file = path+'/swwiki'\n",
    "\n",
    "# 上記のように直接パスやファイル名等を書くのではなく、\n",
    "# 設定ファイルをまとめた JSON ファイル等を経由して指定すると、\n",
    "# ソースコード修正をできるだけ抑えつつ、設定箇所を抽出していることによる可用性が高くなる。\n",
    "config_file = \"config_transfer_learning.json\"\n",
    "with open(config_file, \"r\") as fh:\n",
    "    config = json.load(fh)\n",
    "\n",
    "wikipedia_data = config[\"wikipedia_data\"]\n",
    "path = config[\"model_path\"]\n",
    "model_file = path + \"/\" + config[\"model_base_filename\"]\n",
    "\n",
    "# 時間かかるので、学習済みモデルを上記で指定した場所に保存。\n",
    "# 既に保存済みのモデルの利用にも対応。\n",
    "answer = \"n\"\n",
    "if os.path.exists(model_file):\n",
    "    print(\"# Use model file \\\"{}\\\"?\".format(model_file))\n",
    "    answer = input(\"[y/n] => \")\n",
    "if answer == \"n\":\n",
    "    print(\"loading data...\")\n",
    "    wiki = WikiCorpus(wikipedia_data, lemmatize=False, dictionary={})\n",
    "    sentences = list(wiki.get_texts())\n",
    "\n",
    "    #faxtText\n",
    "    model = FT_gensim(size=200, window=10, min_count=10, workers=max(1, multiprocessing.cpu_count() - 1))\n",
    "\n",
    "    # build the vocabulary\n",
    "    print(\"building vocab...\")\n",
    "    model.build_vocab(sentences=sentences)\n",
    "\n",
    "    # train the model\n",
    "    print(\"training model...\")\n",
    "    model.train(\n",
    "        sentences=sentences, epochs=model.epochs,\n",
    "        total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
    "        window=10, min_count=10,\n",
    "        workers=max(1, multiprocessing.cpu_count() - 1)\n",
    "    )\n",
    "\n",
    "    ## Note: 以下のような pickle.dump では不十分。モデル内部全てを保存するには、用意されてるsave関数を使おう。\n",
    "    #with open(model_file, 'wb') as fh:\n",
    "    #    pickle.dump(model, fh)\n",
    "    model.save(model_file) # 3つの関連ファイルが自動生成。合計約3GB。\n",
    "else:\n",
    "    model = FT_gensim.load(model_file)\n",
    "\n",
    "# 動作確認\n",
    "print(model.wv['artificial'][:5])\n",
    "print(model.wv[\"more like funchuck,Gave this\"][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert from text to vector with the model...\n",
      "# Use train file \"/Users/tnal/PycharmProjects/dm_nlp/transfer_learning/gensim/20news_train.pkl\"?\n",
      "[y/n] => y\n"
     ]
    }
   ],
   "source": [
    "# fine-tuneing stage.\n",
    "# デーセットの用意\n",
    "# こちらも時間かかるので、変換したデータセットを指定した場所に保存。\n",
    "# 既に保存済みデータセットの利用にも対応。\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "train_text = newsgroups_train.data\n",
    "train_label = newsgroups_train.target\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "test_text = newsgroups_test.data\n",
    "test_label = newsgroups_test.target\n",
    "\n",
    "# 事前学習したfastTextにより、文章をベクトルに変換\n",
    "def sentence2vector(sentences, model):\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        vectors.append(model.wv[sent])\n",
    "    return vectors\n",
    "\n",
    "print(\"convert from text to vector with the model...\")\n",
    "train_file = path+\"/\"+config[\"train_file\"]\n",
    "test_file = path+\"/\"+config[\"test_file\"]\n",
    "\n",
    "answer = \"n\"\n",
    "if os.path.exists(train_file):\n",
    "    print(\"# Use train file \\\"{}\\\"?\".format(train_file))\n",
    "    answer = input(\"[y/n] => \")\n",
    "if answer == \"n\":\n",
    "    train_vectors = sentence2vector(train_text, model)\n",
    "    test_vectors = sentence2vector(test_text, model)\n",
    "    with open(train_file, \"wb\") as fh:\n",
    "        pickle.dump(train_vectors, fh)\n",
    "    with open(test_file, \"wb\") as fh:\n",
    "        pickle.dump(test_vectors, fh)\n",
    "else:\n",
    "    with open(train_file, \"rb\") as fh:\n",
    "        train_vectors = pickle.load(fh)\n",
    "    with open(test_file, \"rb\") as fh:\n",
    "        test_vectors = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuneing...\n",
      "0.7854137447405329\n",
      "[[211 108]\n",
      " [ 45 349]]\n"
     ]
    }
   ],
   "source": [
    "# 実際に解決したいタスク（文書分類）を学習するためのモデルを用意し、学習。\n",
    "\n",
    "from sklearn import svm\n",
    "print(\"fine-tuneing...\")\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(train_vectors, train_label)\n",
    "score = clf.score(test_vectors, test_label)\n",
    "print(score)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_label, clf.predict(test_vectors))\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_vectors.shape= (1073, 22464)\n",
      "len(train_label)= 1073\n",
      "test_vectors.shape= (713, 22464)\n",
      "len(test_label)= 713\n",
      "0.5525946704067322\n",
      "[[  0 319]\n",
      " [  0 394]]\n"
     ]
    }
   ],
   "source": [
    "# 比較対象の、事前学習なし実験。\n",
    "# BoW + TFIDFによるベクトル生成\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(\"train_vectors.shape=\", train_vectors.shape)\n",
    "print(\"len(train_label)=\",len(train_label))\n",
    "\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)\n",
    "print(\"test_vectors.shape=\", test_vectors.shape)\n",
    "print(\"len(test_label)=\",len(test_label))\n",
    "\n",
    "# 実際に解決したいタスク（文書分類）を学習するためのモデルを用意し、学習。\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(train_vectors, train_label)\n",
    "score = clf.score(test_vectors, test_label)\n",
    "print(score)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_label, clf.predict(test_vectors))\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
